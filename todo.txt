遗留问题：
1. bert相比原始transformer encoder的变化之处；（等看完transformer源码再回答）
2. bert+crf，crf的作用，是否必要，为什么；搞懂crf；
3. bert book；
4. 两种注意力机制add和dot;
其他面试常问的问题；
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
https://jalammar.github.io/illustrated-transformer/
https://amitness.com/2020/02/albert-visual-summary/