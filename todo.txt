1. pre-training language representations；
3. bert相比原始transformer encoder的变化之处；
消融实验具体是怎么做的。
1.bert+crf，crf的作用，是否必要，为什么；搞懂crf；
2.whole word mask怎么样做的？相比之前的优势是什么？为什么？论文check；


bert book；
两种注意力机制add和dot;
swag具体是怎么做的。
其他rw常问的问题；
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
https://jalammar.github.io/illustrated-transformer/
https://amitness.com/2020/02/albert-visual-summary/